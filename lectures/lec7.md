---
layout: module
title: "Lecture 7: ML systems"
permalink: /lectures/lec7-ml-systems
parent: Lectures
nav_order: 8

---

### Learning objectives:

In this lecture, you will learn:

* **Lec7a:** how the offline Belady caching policy works and why it's optimal
* **Lec7a:** the insight that SHADE builds on
* **Lec7b:** know the concepts of model pruning and quantization
* **Lec7b:** understand how ELF (exponent-less floating-point compression) works


### Lecture slides

* **Lec7a:** Deep learning caching: [slides pdf](/ds5110-cs5501-spring24/assets/docs/lec7a-dl-caching.pdf)
* **Lec7b:** Model compression: [slides pdf](/ds5110-cs5501-spring24/assets/docs/lec7b-ptm-compression.pdf)


### Readings 

* **Lec7a:** [SHADE paper](https://www.usenix.org/conference/fast23/presentation/khan) (*optional*)
* **Lec7b:** [ELF paper](https://arxiv.org/pdf/2402.13429.pdf) (Section 1-4, Section 6.1, *optional*)


### Recordings

* **Lec7a:** [video](https://edstem.org/us/courses/53518/discussion/4593216)
* **Lec7b:** [video](https://edstem.org/us/courses/53518/discussion/4620768)


### Quiz

* [Quiz 6](https://forms.gle/9nDT5FS3dej2cVQt6)

